# FastAPI Configuration
DEBUG=true
HOST=0.0.0.0
PORT=8000

# Slack Configuration (Optional - for Slack integration)
# For Socket Mode (development without ngrok):
SLACK_BOT_TOKEN=xoxb-your-slack-bot-token
SLACK_APP_TOKEN=xapp-your-slack-app-token

# For Webhook Mode (with ngrok or production):
SLACK_BOT_TOKEN=xoxb-your-slack-bot-token
SLACK_SIGNING_SECRET=your-slack-signing-secret

# See docs/SLACK_SETUP.md for Socket Mode setup
# See docs/NGROK_WEBHOOK_SETUP.md for Webhook Mode with ngrok

# AI Configuration - Multi-Provider Support
# Choose your LLM provider: anthropic, openai, gemini, groq, ollama
LLM_PROVIDER=anthropic

# API Keys (only set the key for your chosen provider)
# Get Anthropic API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key

# Get OpenAI API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key

# Get Google API key from: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=your_google_api_key

# Get Groq API key from: https://console.groq.com/
GROQ_API_KEY=your_groq_api_key

# Model configuration - set according to your provider
# Anthropic models: claude-sonnet-4-5-20250929, claude-3-5-sonnet-20241022, claude-3-opus-20240229
# OpenAI models: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
# Gemini models: gemini-1.5-pro, gemini-1.5-flash, gemini-2.0-flash-exp
# Groq models: llama-3.1-70b-versatile, mixtral-8x7b-32768, llama-3.1-8b-instant
# Ollama models: llama2, mistral, codellama (local models)
MODEL_NAME=claude-sonnet-4-5-20250929

MAX_TOKENS=4096
TEMPERATURE=0.7

DATABASE_URL=postgresql://rum:Rum1234@localhost:2345/rum

# Feature Flags
ENABLE_DAILY_STANDUP=true
ENABLE_SPRINT_PLANNING=true
ENABLE_RETROSPECTIVES=true

# CORS Settings (JSON array format)
CORS_ORIGINS=["*"]
