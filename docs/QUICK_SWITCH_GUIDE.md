# Quick Provider Switch Guide

H∆∞·ªõng d·∫´n nhanh ƒë·ªÉ chuy·ªÉn ƒë·ªïi gi·ªØa c√°c AI providers.

## üéØ C√°ch Chuy·ªÉn Provider

### B∆∞·ªõc 1: ƒêi·ªÅn API Keys v√†o `.env`

M·ªü file `.env` v√† ƒëi·ªÅn API keys c·ªßa b·∫°n:

```bash
# Ch·ªâ c·∫ßn ƒëi·ªÅn key cho providers b·∫°n mu·ªën d√πng
ANTHROPIC_API_KEY=sk-ant-xxx...
OPENAI_API_KEY=sk-xxx...
GOOGLE_API_KEY=AIzaSy-xxx...
GROQ_API_KEY=gsk_xxx...
```

### B∆∞·ªõc 2: Ch·ªçn Provider

Trong file `.env`, thay ƒë·ªïi 2 d√≤ng sau:

```bash
LLM_PROVIDER=anthropic  # ƒê·ªïi th√†nh: anthropic, openai, gemini, groq, ollama
MODEL_NAME=claude-sonnet-4-5-20250929  # ƒê·ªïi model t∆∞∆°ng ·ª©ng
```

### B∆∞·ªõc 3: Restart Server

```bash
# Ctrl+C ƒë·ªÉ d·ª´ng server hi·ªán t·∫°i
python main.py
```

## üìã Quick Switch Examples

Copy v√† paste c√°c v√≠ d·ª• sau v√†o file `.env` c·ªßa b·∫°n:

### 1Ô∏è‚É£ Anthropic Claude Sonnet 4.5 (M·∫∑c ƒë·ªãnh - Ch·∫•t l∆∞·ª£ng cao nh·∫•t)

```bash
LLM_PROVIDER=anthropic
MODEL_NAME=claude-sonnet-4-5-20250929
```

**Khi n√†o d√πng:** C·∫ßn ch·∫•t l∆∞·ª£ng t·ªët nh·∫•t, c√≥ ng√¢n s√°ch v·ª´a ph·∫£i

---

### 2Ô∏è‚É£ OpenAI GPT-4o (Ti√™u chu·∫©n ng√†nh)

```bash
LLM_PROVIDER=openai
MODEL_NAME=gpt-4o
```

**Khi n√†o d√πng:** C·∫ßn s·ª± ·ªïn ƒë·ªãnh, ƒë√£ quen v·ªõi OpenAI

---

### 3Ô∏è‚É£ OpenAI GPT-4o Mini (Nhanh & R·∫ª)

```bash
LLM_PROVIDER=openai
MODEL_NAME=gpt-4o-mini
```

**Khi n√†o d√πng:** C·∫ßn ti·∫øt ki·ªám chi ph√≠, tasks ƒë∆°n gi·∫£n
**Chi ph√≠:** R·∫ª h∆°n GPT-4o **95%** (~$0.15/$0.60 per 1M tokens)

---

### 4Ô∏è‚É£ Google Gemini Pro (Ch·∫•t l∆∞·ª£ng cao, gi√° t·ªët)

```bash
LLM_PROVIDER=gemini
MODEL_NAME=gemini-1.5-pro
```

**Khi n√†o d√πng:** C·∫ßn ch·∫•t l∆∞·ª£ng t·ªët v·ªõi gi√° r·∫ª h∆°n
**Chi ph√≠:** ~$1.25/$5.00 per 1M tokens

---

### 5Ô∏è‚É£ Google Gemini Flash (R·∫•t nhanh & r·∫ª)

```bash
LLM_PROVIDER=gemini
MODEL_NAME=gemini-1.5-flash
```

**Khi n√†o d√πng:** C·∫ßn t·ªëc ƒë·ªô cao, chi ph√≠ th·∫•p
**Chi ph√≠:** C·ª±c r·∫ª ~$0.075/$0.30 per 1M tokens
**T·ªëc ƒë·ªô:** R·∫•t nhanh, ph√π h·ª£p cho production

---

### 6Ô∏è‚É£ Groq Llama 3.1 70B (Mi·ªÖn ph√≠ & si√™u nhanh)

```bash
LLM_PROVIDER=groq
MODEL_NAME=llama-3.1-70b-versatile
```

**Khi n√†o d√πng:** C·∫ßn t·ªëc ƒë·ªô c·ª±c nhanh, c√≥ free tier
**Chi ph√≠:** FREE tier c√≥ s·∫µn!
**T·ªëc ƒë·ªô:** Si√™u nhanh (~300 tokens/s)

---

### 7Ô∏è‚É£ Ollama Local (Mi·ªÖn ph√≠, ri√™ng t∆∞, offline)

```bash
LLM_PROVIDER=ollama
MODEL_NAME=llama2
```

**Setup tr∆∞·ªõc khi d√πng:**
```bash
# 1. C√†i Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Pull model
ollama pull llama2

# 3. Ch·∫°y Ollama
ollama serve
```

**Khi n√†o d√πng:**
- C·∫ßn privacy tuy·ªát ƒë·ªëi
- Kh√¥ng mu·ªën g·ª≠i data ra ngo√†i
- Kh√¥ng c√≥ internet ho·∫∑c mu·ªën ti·∫øt ki·ªám 100% chi ph√≠
- C√≥ GPU m·∫°nh ƒë·ªÉ ch·∫°y local

---

## üí° Tips Chuy·ªÉn Provider

### Test Nhanh M·ªôt Provider

Kh√¥ng c·∫ßn restart server, ch·ªâ c·∫ßn:

1. Stop server (Ctrl+C)
2. S·ª≠a `.env`:
   ```bash
   LLM_PROVIDER=gemini
   MODEL_NAME=gemini-1.5-flash
   ```
3. Start l·∫°i: `python main.py`
4. Test API: `curl http://localhost:8000/api/v1/crewai/test`

### So S√°nh Providers

ƒê·ªÉ test c√πng m·ªôt request v·ªõi nhi·ªÅu providers:

```bash
# 1. Test v·ªõi Anthropic
LLM_PROVIDER=anthropic
MODEL_NAME=claude-sonnet-4-5-20250929
# Ch·∫°y test, ghi l·∫°i k·∫øt qu·∫£

# 2. Test v·ªõi Gemini
LLM_PROVIDER=gemini
MODEL_NAME=gemini-1.5-pro
# Ch·∫°y test, so s√°nh k·∫øt qu·∫£

# 3. Test v·ªõi Groq (nhanh nh·∫•t)
LLM_PROVIDER=groq
MODEL_NAME=llama-3.1-70b-versatile
# Ch·∫°y test, so s√°nh t·ªëc ƒë·ªô
```

## üìä So S√°nh Nhanh

| Provider | Model | Chi ph√≠ | T·ªëc ƒë·ªô | Ch·∫•t l∆∞·ª£ng | Use Case |
|----------|-------|---------|--------|------------|----------|
| Anthropic | Sonnet 4.5 | $$ | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Production, critical tasks |
| OpenAI | GPT-4o | $$ | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Production, familiar |
| OpenAI | GPT-4o Mini | $ | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Cost-effective |
| Gemini | Pro | $ | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Best value |
| Gemini | Flash | $ | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | High volume |
| Groq | Llama 3.1 | FREE | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Speed critical |
| Ollama | Local | FREE | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Privacy, offline |

## üîß Troubleshooting

### L·ªói: "API key for X is not set"

```bash
# Check .env c√≥ ƒë√∫ng key ch∆∞a
cat .env | grep ANTHROPIC_API_KEY

# Ph·∫£i c√≥ key th·∫≠t, kh√¥ng ph·∫£i placeholder
ANTHROPIC_API_KEY=sk-ant-xxx  # ‚úì ƒê√∫ng
ANTHROPIC_API_KEY=your_key     # ‚úó Sai - v·∫´n l√† placeholder
```

### L·ªói: "Authentication failed"

- Check API key c√≥ ƒë√∫ng kh√¥ng
- Check API key c√≥ h·∫øt h·∫°n ho·∫∑c h·∫øt credits kh√¥ng
- Verify l·∫°i tr√™n trang console c·ªßa provider

### Ollama kh√¥ng ch·∫°y

```bash
# Check Ollama c√≥ ƒëang ch·∫°y kh√¥ng
ollama list

# N·∫øu ch∆∞a ch·∫°y, start Ollama
ollama serve

# Pull model n·∫øu ch∆∞a c√≥
ollama pull llama2
```

## üìö T√†i Li·ªáu Chi Ti·∫øt

Xem th√™m: [docs/LLM_PROVIDERS.md](LLM_PROVIDERS.md)
